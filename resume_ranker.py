# -*- coding: utf-8 -*-
"""resume_ranker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_4CkJxQeAjofrzDWU0qRzC3xv8tfRxYP
"""

import pandas as pd
import re
import nltk
import numpy as np
from nltk.corpus import wordnet
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics.pairwise import cosine_similarity
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import stopwords as stp
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

!pip install gradio

from google.colab import drive
drive.mount('/content/drive')

!pip install typing_extensions

import gradio as gr

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Resume Ranking Data Set.csv')
df_cp = df.copy()

df_names = df['name'].copy()

df.isnull().sum()

df.drop(['accomplishments_segment','education_segment','emails','misc_segment','name','objectives_segment','phone','projects_segment','skills_segment','text',
         'university_0','university_1','university_2','university_3','university_4','university_5','url','work_segment'],axis=1,inplace=True)
df.shape

print('After removing the columns from dataset..')
df.isnull().sum()

df['degree'].fillna(df.degree.mode()[0],inplace=True)
df['links'].fillna('Missing',inplace=True)
df['work_experience'].fillna(0,inplace=True)
df['job_titles'].fillna('No Job',inplace=True)

all_degrees = ''
for i in df.degree:
  if len(all_degrees) == 0:
    all_degrees = i
  else:
    all_degrees = all_degrees + ' , ' + i

all_degrees = all_degrees.split(',')
all_degrees = [re.sub("[\s.]","",i).upper() for i in all_degrees]
unique_degrees = set(all_degrees)
unique_degrees

df['bachelor_degrees'] = 'No Degree'
df['master_degrees'] = 'No Degree'
df['docterte_degrees'] = 'No Degree'
df['profiles'] = 'No Profile'

ind = 0
for i in df.degree:
  lst = re.sub("[\s.]","",i).upper( )
  # print(lst)
  for j in lst.split(','):
    if j in ['BE', 'BS', 'BSC', 'BTECH']:
      if df.loc[ind,'bachelor_degrees'] == 'No Degree':
        df.loc[ind,'bachelor_degrees'] = j
      else:
        df.loc[ind,'bachelor_degrees'] = df.loc[ind,'bachelor_degrees'] + ' , '+ j
    elif j in ['ME', 'MS', 'MSC', 'MTECH']:
      if df.loc[ind,'master_degrees'] == 'No Degree':
        df.loc[ind,'master_degrees'] = j
      else:
        df.loc[ind,'master_degrees'] = df.loc[ind,'master_degrees'] + ' , '+ j
    elif j == 'PHD':
      if df.loc[ind,'docterte_degrees'] == 'No Degree':
        df.loc[ind,'docterte_degrees'] = j
      else:
        df.loc[ind,'docterte_degrees'] = df.loc[ind,'docterte_degrees'] + ' , '+ j
  ind +=1

df

ind = 0
for i in df.links:
  lst = re.sub("[\s]","",i)
  # print(lst)
  for j in lst.split(','):
    if j.find('github') != -1:
      df.loc[ind,'profiles'] = 'Github'
    elif j.find('linkedin') != -1:
      if df.loc[ind,'profiles'] == 'No Profile':
        df.loc[ind,'profiles'] = 'Linkedin'
      else:
        df.loc[ind,'profiles'] = df.loc[ind,'profiles'] + ' , ' + 'Linkedin'
  ind +=1

ind = 0
for i in df.work_experience:
  if i < 0:
    df.loc[ind,'work_experience'] = i*-1
  elif i > 40:
    df.loc[ind,'work_experience'] = 0
  ind +=1
df['work_experience'] = df['work_experience'].astype(int)

with open('/content/drive/MyDrive/Colab Notebooks/Resume Ranking Data Set.csv','r',encoding = 'utf-8') as f:
  file_desc_lst =  [r.replace('\n', '') for r in f.readlines()]

job_description = ''

for i in file_desc_lst:
  if len(job_description) == 0:
    job_description = str(i)
  else:
    job_description = job_description + ' ' + str(i)
print(job_description)

all_resume_text = []

for i in df.iloc[:].values:
  s = ''
  for j in list(i):
    if len(s) == 0:
      s = str(j)
    else:
      s = s + ' , ' + str(j)
  all_resume_text.append(s)

lemmatizer = WordNetLemmatizer()
analyzer = CountVectorizer().build_analyzer()

def stemmed_words(doc):
    return (lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in analyzer(doc) if w not in set(stp.words('english')))

def get_wordnet_pos(word):
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)

def get_tf_idf_cosine_similarity(job_desc, all_resumes):
    tf_idf_vect = TfidfVectorizer(analyzer=stemmed_words)
    job_description = [job_desc]
    tf_idf_desc_vector = np.array(tf_idf_vect.fit_transform(job_description).todense())
    tf_idf_desc_vector = tf_idf_desc_vector.reshape(1, -1)
    tf_idf_resume_vector = np.array(tf_idf_vect.transform(all_resumes).todense())
    cosine_similarity_list = []
    for resume_vector in tf_idf_resume_vector:
      cosine_similarity_list.append(cosine_similarity(tf_idf_desc_vector, resume_vector.reshape(1,-1))[0][0])
    return cosine_similarity_list

import nltk
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

import numpy as np

import nltk

# Download the required data for the averaged perceptron tagger
nltk.download('averaged_perceptron_tagger_eng')

# Download any other required resources as well
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

# ... rest of the code ...
cos_sim_list = get_tf_idf_cosine_similarity(job_description, all_resume_text)

zipped_resume_rating = zip(cos_sim_list,df_cp.name,[x for x in range(len(df))])
sorted_resume_rating_list = sorted(zipped_resume_rating, key = lambda x: x[0], reverse=True)

resume_score = [round(x*100,2) for x in cos_sim_list]
pd.concat([df_cp.name,pd.DataFrame(resume_score,columns=['resume_score(%)'])],axis=1).sort_values(by=['resume_score(%)'],ascending=False).head(10)

# Encoding categorical variables
label_encoder = LabelEncoder()
df['bachelor_degrees'] = label_encoder.fit_transform(df['bachelor_degrees'])
df['master_degrees'] = label_encoder.fit_transform(df['master_degrees'])
df['docterte_degrees'] = label_encoder.fit_transform(df['docterte_degrees'])
df['profiles'] = label_encoder.fit_transform(df['profiles'])

# Extracting features and target variable
X = df[['bachelor_degrees', 'master_degrees', 'docterte_degrees', 'profiles', 'work_experience']]
y = cos_sim_list  # Using cosine similarity as the target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Initialize the Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train)


# Encoding categorical variables
label_encoder = LabelEncoder()
df['bachelor_degrees'] = label_encoder.fit_transform(df['bachelor_degrees'])
df['master_degrees'] = label_encoder.fit_transform(df['master_degrees'])
df['docterte_degrees'] = label_encoder.fit_transform(df['docterte_degrees'])
df['profiles'] = label_encoder.fit_transform(df['profiles'])

# Extracting features and target variable
X = df[['bachelor_degrees', 'master_degrees', 'docterte_degrees', 'profiles', 'work_experience']]
y = cos_sim_list  # Using cosine similarity as the target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
!pip install scikit-learn-intelex
from sklearnex import patch_sklearn
patch_sklearn()
# Initialize the Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Calculate Mean Squared Error
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print(results_df.head(10))
resume_score = [round(x*100,2) for x in cos_sim_list]
pd.concat([df_cp.name,pd.DataFrame(resume_score,columns=['resume_score(%)'])],axis=1).sort_values(by=['resume_score(%)'],ascending=False).head(10)


from sklearn.metrics import r2_score

# Calculate R-squared
r2 = r2_score(y_test, y_pred)

# Convert R-squared to percentage
accuracy_percentage = r2 * 100

print(f'R-squared: {r2}')
print(f'Accuracy: {accuracy_percentage:.2f}%')

def get_resume_score(job_description, df, all_resume_text):
    cos_sim_list = get_tf_idf_cosine_similarity(job_description, all_resume_text)

    # Encode categorical variables
    label_encoder = LabelEncoder()
    df['bachelor_degrees'] = label_encoder.fit_transform(df['bachelor_degrees'])
    df['master_degrees'] = label_encoder.fit_transform(df['master_degrees'])
    df['docterte_degrees'] = label_encoder.fit_transform(df['docterte_degrees'])
    df['profiles'] = label_encoder.fit_transform(df['profiles'])

    # Extract features and target variable
    X = df[['bachelor_degrees', 'master_degrees', 'docterte_degrees', 'profiles', 'work_experience']]
    y = cos_sim_list

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # If running in a notebook, you might use the following command; otherwise, ensure scikit-learn-intelex is installed:
    # %pip install scikit-learn-intelex
    from sklearnex import patch_sklearn
    patch_sklearn()

    # Initialize the Random Forest Regressor
    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

    # Train the model
    rf_model.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = rf_model.predict(X_test)

    # Calculate Mean Squared Error
    mse = mean_squared_error(y_test, y_pred)
    print(f'Mean Squared Error: {mse}')
    results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
    print(results_df.head(10))

    # Calculate R-squared
    r2 = r2_score(y_test, y_pred)
    accuracy_percentage = r2 * 100

    print(f'R-squared: {r2}')
    print(f'Accuracy: {accuracy_percentage:.2f}%')

    # Multiply cosine similarities by 100
    cos_sim_list = [x * 100 for x in cos_sim_list]
    return pd.concat([df_names, pd.DataFrame(cos_sim_list, columns=['resume_score(%)'])],
                     axis=1).sort_values(by=['resume_score(%)'], ascending=False).head(10)


# Gradio interface function
def gr_interface(job_description_file):
    with open(job_description_file.name, 'r', encoding='utf-8') as f:
        file_desc_lst = [r.replace('\n', '') for r in f.readlines()]

    job_description = ''
    # Fixed loop: removed extra '00'
    for i in file_desc_lst:
        if len(job_description) == 0:
            job_description = str(i)
        else:
            job_description = job_description + ' ' + str(i)

    all_resume_text = []
    for i in df.iloc[:].values:
        s = ''
        for j in list(i):
            if len(s) == 0:
                s = str(j)
            else:
                s = s + ' , ' + str(j)
        all_resume_text.append(s)

    top_resumes = get_resume_score(job_description, df, all_resume_text)
    table_html = top_resumes.reset_index().to_html(index=False)

    model_explanation = """
    <div class="model-explanation">
        <h3>Model Explanation</h3>
        <p>This model uses a Random Forest Regressor to score resumes based on their similarity to the provided job description.
        It considers various features such as education, profiles, and work experience to make predictions.</p>
    </div>
    """

    # Concatenate the model explanation and the table HTML
    full_html = model_explanation + table_html
    return full_html


iface = gr.Interface(
    fn=gr_interface,
    inputs=gr.File(label="Upload Job Description", type="filepath"),
    outputs=gr.HTML(label="Top 10 Resumes"),
    theme="huggingface",
    live=True,
    title="Resume Scorer",
    description="Upload a job description file and get the top 10 matching resumes.",
    css="""
    .gr-outer-container {
        max-width: 800px;
        margin: auto;
        background-color: #f4f4f4;  /* Set your desired background color */
        font-family: 'Arial', sans-serif;  /* Set your desired font */
    }
    .gr-interface {
        padding: 20px;
        border: 1px solid #ccc;
        border-radius: 10px;
        box-shadow: 0px 0px 10px 0px #ccc;  /* Add box shadow for a nice effect */
    }
    .gr-model {
        margin-bottom: 20px;
    }
    .gr-output {
        background-color: #fff;  /* Set the background color of the output area */
        padding: 10px;
        border-radius: 5px;
    }
    .gr-model-title {
        font-size: 20px;  /* Set the font size of the model title */
        color: #333;  /* Set the font color of the model title */
    }
    """,
)

# Launch the Gradio interface
iface.launch(debug=True)

"""Thank You
Done By Manikandan M
"""